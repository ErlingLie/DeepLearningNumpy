{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer\n",
    "\n",
    "\n",
    "**Task:** \n",
    "\n",
    "* **Given**\n",
    "\n",
    "The function for the cross-correlation which is used in Conv-nets\n",
    "$$P_{c,x,y} = b + \\sum_{d = 0}^{D}\\sum_{i = -\\frac{k}{2}}^{\\frac{k}{2}} \\sum_{j = -\\frac{k}{2}}^{\\frac{k}{2}} w_{c,d,i,j}*X_{d,x+i,y+j}$$\n",
    "\n",
    "Where c is the filter index of the output feature map vector, D is the number of feature-maps in the input and k is the size of the convolution kernel. $P_{c,x,y}$ is one pixel in one feature map c.\n",
    "\n",
    "* **Find** \n",
    "    * The equations for backward pass in a convolutional net\n",
    "    * A numerical forward pass that qualitatively seems reasonable\n",
    "    * A numerical solution to a forward and backward pass in a convolutional net that proves the correctness of the gradients.\n",
    "        * The solution should demonstrate how change of depth and height/width dimension is handled in a convolutional net. Both by strided convolution and by maxpool\n",
    "        * The gradients should be verified with an appropriate method.\n",
    "        * For simplicity the L2 loss function is **given** $$L2 = \\frac{1}{N} \\sum_{i = 0}^{N} \\frac{1}{2} (\\hat{y_i}-y_i)^2$$, $$\\frac{\\partial L2}{\\partial \\hat{y_i}} = \\frac{1}{N}(\\hat{y_i} - y_i)$$\n",
    "\n",
    "\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "The derivative of a pixel in the output feature map, with respect to an input pixel X_{d, x+i,y+j} will the be this:\n",
    "\n",
    "$$\\frac{\\partial P_{c,x,y}}{\\partial X_{d,x + i,y + j}} =  w_{c,d,i,j}$$\n",
    "\n",
    "given that i and j are within within the kernel size, else the derivative is zero. Thus, to backpropagate, one must calculate the derivative of every pixel, in every feature map of the output, with respect to every pixel in every feature map of the input.\n",
    "\n",
    "The derivative of a pixel in the output feature map, with respect to the weights is:\n",
    "\n",
    "$$\\frac{\\partial P_{c,x,y}}{\\partial w_{c,d,i,j}} =  X_{d, x+i, y+j}$$\n",
    "\n",
    "To get the full derivative of the weights one must sum over all pixels, in all output channels.\n",
    "\n",
    "The derivative of the bias is of course simply 1.\n",
    "\n",
    "\n",
    "When we have the derivatives of the Convolutional layer, we can use the chain rule to backpropagate the loss through several layers. This code will illustrate a convolutional network fully working with multiple feature maps, configurable stride and padding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definitions\n",
    "\n",
    "The following model defines a convolutional network with batch sizes of 1, or in other words no batches at all only singular examples. The model allows for variable input dimension and output dimension, and allows strided convolutions for downsampling. The ReLU activation function is defined, but could easily be switched out with any other activation function.\n",
    "\n",
    "I have chosen to only provide this numerical example in python/numpy and not hand-calculated, as doing image convolutions by hand is a very tedious process.\n",
    "\n",
    "The code for the network is a bit larger than I would like for a simple illustrative example, but every code line seems to be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolutional layer\n",
    "This convolutional layer class implements a forward pass and backpropegation. The class saves the input feature maps $X$ as cache during the forward pass, as it is needed for the backward pass as stated in the equations above.\n",
    "\n",
    "The interesting parts of this class is the line that calculates the output size of a convolutional layer\n",
    "\n",
    "$$W_y = \\frac{W_x - K + 2 P_w}{Stride} + 1$$\n",
    "\n",
    ", and the actual forward and backward passes that are contained within the for-loops.\n",
    "\n",
    "\n",
    "\n",
    "The code is inspired by https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199, however several changes are made. This conv layer has added the functionality of padding, bias, more channels and strided convolution. It might be that adding these things reduces the simplicity of the example, but I believe the additional insight they provide outweigh the increased complexity of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convLayer():\n",
    "    def __init__(self, kernelsize, inputDim, outDim, stride=1, pad=0):\n",
    "        self.kernelsize = kernelsize\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.in_channels = inputDim\n",
    "        self.out_channels = outDim\n",
    "        \n",
    "        self.W = np.random.normal(0, 1, (outDim, inputDim, kernelsize, kernelsize))\n",
    "        #self.bias = np.random.normal(0,1,(outDim))\n",
    "        \n",
    "        self.cache = None\n",
    "        \n",
    "        self.gradW = None\n",
    "        #self.gradB = None\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        (D, x_width, x_height) = x.shape\n",
    "        x = np.pad(x, (((0,0),(self.pad, self.pad),(self.pad,self.pad))), 'constant')\n",
    "        \n",
    "        output_size = (x_width - self.kernelsize + 2*self.pad)//self.stride + 1\n",
    "        \n",
    "        output = np.zeros((self.out_channels, output_size, output_size))\n",
    "        stride = self.stride\n",
    "        \n",
    "        '''\n",
    "        The following loop performes the equation described above. Instead of looping over all input channels, \n",
    "        I apply vectorisation with numpy.\n",
    "        '''\n",
    "        for i in range(output.shape[1]):\n",
    "            for j in range(output.shape[2]):\n",
    "                x_slice = x[:,i*stride:i*stride+self.kernelsize, j*stride:j*stride + self.kernelsize]\n",
    "                output[:,i,j] += np.sum(self.W * x_slice, (1,2,3))# + self.bias   \n",
    "        self.cache = x\n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        x = self.cache\n",
    "\n",
    "        (C,H, W) = delta.shape\n",
    "    \n",
    "        dX = np.zeros(x.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        #dB = np.sum(delta, axis = (1,2))\n",
    "        stride = self.stride\n",
    "        # Looping over all pixels in the output. For all pixels and all channels the derivative of this layer is\n",
    "        # calculated according to the above equations, and multiplied with delta according to the chain rule.\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                dx = self.W * delta[:,h,w].reshape(C,-1,1,1)\n",
    "                dX[:,h*stride:h*stride+self.kernelsize, w*stride:w*stride+self.kernelsize] += np.sum(dx,axis=0)\n",
    "                \n",
    "                x_slice = x[:,h*stride:h*stride+self.kernelsize, w*stride:w*stride+self.kernelsize]\n",
    "                dW +=  x_slice* delta[:,h,w].reshape(C,-1,1,1)\n",
    "    \n",
    "        self.gradW = dW\n",
    "        #self.gradB = dB\n",
    "        return dX[:,self.pad:-self.pad, self.pad:-self.pad]\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pool\n",
    "\n",
    "The following class implements max pooling. Max Pool is a common method for down sampling the image h/w dimensions. A 2x2 grid is slided over the image in strides of 2, picking out the max value at each 2x2 section.\n",
    "\n",
    "The **gradient** will be 1 every place where the max values originate, and 0 elsewhere. Therefor, the code saves a \"bed of nails\" image as cache, which keeps memory of which indexes provided the max values. Backward pass is then simply multiplying the delta with each appropriate 2x2 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool():\n",
    "    '''\n",
    "    This function performs maxpooling with stride of 2 and kernel size 2.\n",
    "    It saves a ''needle map'' consisting of ones where the maximum value was taken from, and zero elsewhere.\n",
    "    The backprop is simply multiplying delta with the needle map.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    def __call__(self, x):\n",
    "        self.cache = np.zeros(x.shape)\n",
    "        output = np.zeros((x.shape[0],(x.shape[1]+1)//2, (x.shape[2]+1)//2))\n",
    "        for i in range(output.shape[1]):\n",
    "            for j in range(output.shape[2]):\n",
    "                x_slice = x[:,i*2:i*2+2, j*2:j*2+2]\n",
    "                output[:,i,j] = np.amax(x_slice, axis=(1,2))\n",
    "   \n",
    "                idx_map = np.where(x_slice == output[:,i,j].reshape(-1,1,1),\\\n",
    "                                   np.ones((output.shape[0],2,2)), np.zeros((output.shape[0],2,2)))\n",
    "                self.cache[:,i*2:i*2+2, j*2:j*2+2] = idx_map\n",
    "        return output\n",
    "    def backward(self, d):\n",
    "        output = np.zeros(self.cache.shape)\n",
    "        for i in range(d.shape[1]):\n",
    "            for j in range(d.shape[2]):\n",
    "                self.cache[:,i*2:i*2+2,j*2:j*2+2]*= d[:,i,j].reshape(-1,1,1)\n",
    "        return self.cache\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    def __call__(self, x):\n",
    "        y = np.maximum(x,0.)\n",
    "        self.cache = y\n",
    "        return y\n",
    "    def backward(self,y):\n",
    "        return np.where(self.cache >0, 1,0)*y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l2_loss():\n",
    "    '''\n",
    "    For simplicity the l2_loss function is hard-coded for image shape of 28x28\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    def __call__(self, x, y):\n",
    "        self.cache = (x-y)/784\n",
    "        return np.sum(0.5*(x-y)**2)/784\n",
    "    def backward(self):\n",
    "        return self.cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward function\n",
    "This code will demonstrate the forward function, with a couple of known kernel weights. Note that the convoluion is strided, and as expected, the output feature size is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-1.      0.      1.    ]\n",
      "   [-2.      0.      2.    ]\n",
      "   [-1.      0.      1.    ]]]\n",
      "\n",
      "\n",
      " [[[ 0.      0.      0.    ]\n",
      "   [ 0.      0.      0.    ]\n",
      "   [ 0.      0.      0.    ]]]\n",
      "\n",
      "\n",
      " [[[ 0.0625  0.125   0.0625]\n",
      "   [ 0.125   0.25    0.125 ]\n",
      "   [ 0.0625  0.125   0.0625]]]]\n",
      "We can see that the result of the forward pass is as expected\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAKx0lEQVR4nO3df6zddX3H8edrrajgDDCjwZYNSIibIU5MY1DMRsQlnRLKH1uCmUkzlzQmW0SzRWFkWfbH/tIs8seypenQZhLIgqiETAdhZu4fGgpsWChChw4qlbKxqZl/QON7f9zT7Xrtr5zvj3vT9/ORNPecc88578+96TPf7zm9vZ9UFZLOfj+33guQNA9jl5owdqkJY5eaMHapic1zDkviW//SxKoqJ7rdI7vUhLFLTRi71ISxS00Mij3J9iTfTnIoyc1jLUrS+LLsz8Yn2QQ8DfwGcBh4GPhwVT15isf4brw0sSnejX83cKiqnq2qV4C7gB0Dnk/ShIbEvgV4ftX1w4vbfkqSXUn2J9k/YJakgYb8UM2JThV+5jS9qnYDu8HTeGk9DTmyHwYuXnV9K/DCsOVImsqQ2B8GLk9yaZJzgBuBe8dZlqSxLX0aX1XHkvwB8A/AJuD2qnpitJVJGtXS//S21DBfs0uT8z/CSM0Zu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUxNKxJ7k4yTeSHEzyRJKbxlyYpHEN2bL5IuCiqno0yc8DjwA3uGWztL5G/73xVXWkqh5dXP4RcJAT7OIqaWMYsovr/0lyCXAlsO8En9sF7BpjjqTlDd7+KckbgH8C/ryq7jnNfT2NlyY2yfZPSV4DfAm443ShS1pfQ96gC7AXeLmqPnGGj/HILk3sZEf2IbG/D/hn4FvATxY3/3FV/f0pHmPs0sRGj30Zxi5Nzy2bpeaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qYnDsSTYleSzJfWMsSNI0xjiy38TKDq6SNrChe71tBT4E7BlnOZKmMvTI/jngU/z/9k8/I8muJPuT7B84S9IAS8ee5DrgaFU9cqr7VdXuqtpWVduWnSVpuCFH9quB65N8F7gLeH+SL46yKkmjG2VjxyTXAH9UVded5n5u7ChNzI0dpebcslk6y3hkl5ozdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapiaEbO56f5O4kTyU5mOQ9Yy1M0rg2D3z8bcDXq+q3kpwDnDvCmiRNYOlNIpK8EfhX4LI6wydxkwhpelNsEnEZ8BLw+SSPJdmT5Ly1d3LLZmljGHJk3wY8BFxdVfuS3Ab8sKr+5BSP8cguTWyKI/th4HBV7Vtcvxt414DnkzShpWOvqu8Dzyd52+Kma4EnR1mVpNEN2sU1yTuBPcA5wLPA71bVf53i/p7GSxM72Wm8WzZLZxm3bJaaM3apCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qYmhWzZ/MskTSQ4kuTPJ68ZamKRxLR17ki3Ax4FtVXUFsAm4cayFSRrX0NP4zcDrk2xmZW/2F4YvSdIUhuz19j3gs8BzwBHgB1V1/9r7uWWztDEMOY2/ANgBXAq8FTgvyUfW3q+qdlfVtqratvwyJQ015DT+A8B3quqlqnoVuAd47zjLkjS2IbE/B1yV5NwkYWXL5oPjLEvS2Ia8Zt8H3A08Cnxr8Vy7R1qXpJG5ZbN0lnHLZqk5Y5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjht7EluT3I0yYFVt12Y5IEkzyw+XjDtMiUNdSZH9i8A29fcdjPwYFVdDjy4uC5pAztt7FX1TeDlNTfvAPYuLu8Fbhh5XZJGtnnJx72lqo4AVNWRJG8+2R2T7AJ2LTlH0kiWjf2MVdVuFnvAuf2TtH6WfTf+xSQXASw+Hh1vSZKmsGzs9wI7F5d3Al8dZzmSpnLaXVyT3AlcA7wJeBH4U+ArwN8Bv8jKPu2/XVVr38Q70XN5Gi9N7GS7uLpls3SWcctmqTljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaWHbL5s8keSrJ40m+nOT8aZcpaahlt2x+ALiiqt4BPA3cMvK6JI1sqS2bq+r+qjq2uPoQsHWCtUka0Riv2T8KfG2E55E0oUFbNie5FTgG3HGK+7g/u7QBnNFeb0kuAe6rqitW3bYT+BhwbVX9+IyGudebNLmT7fW21JE9yXbg08Cvn2noktbXsls23wK8FvjPxd0eqqqPnXaYR3Zpcm7ZLDXhls1Sc8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41MehXSS/hP4B/P8Xn37S4z3pwtrPPhtm/dLJPzPo76E4nyf6q2uZsZzt7fJ7GS00Yu9TERot9t7Od7expbKjX7JKms9GO7JImYuxSExsi9iTbk3w7yaEkN8849+Ik30hyMMkTSW6aa/aqNWxK8liS+2aee36Su5M8tfj63zPj7E8uvt8HktyZ5HUTz7s9ydEkB1bddmGSB5I8s/h4wYyzP7P4vj+e5MtJzp9i9lrrHnuSTcBfAr8JvB34cJK3zzT+GPCHVfUrwFXA7884+7ibgIMzzwS4Dfh6Vf0y8KtzrSHJFuDjwLbFFuCbgBsnHvsFYPua224GHqyqy4EHF9fnmv0AcEVVvQN4mpWNUie37rED7wYOVdWzVfUKcBewY47BVXWkqh5dXP4RK3/ht8wxGyDJVuBDwJ65Zi7mvhH4NeBvAKrqlar67xmXsBl4fZLNwLnAC1MOq6pvAi+vuXkHsHdxeS9ww1yzq+r+qjq2uPoQsHWK2WtthNi3AM+vun6YGYM7LsklwJXAvhnHfg74FPCTGWcCXAa8BHx+8RJiT5Lz5hhcVd8DPgs8BxwBflBV988xe423VNWRxZqOAG9ehzUAfBT42hyDNkLsJ9pedtZ/D0zyBuBLwCeq6oczzbwOOFpVj8wxb43NwLuAv6qqK4H/YbrT2J+yeG28A7gUeCtwXpKPzDF7o0lyKysvJe+YY95GiP0wcPGq61uZ+LRutSSvYSX0O6rqnrnmAlcD1yf5LisvXd6f5IszzT4MHK6q42cxd7MS/xw+AHynql6qqleBe4D3zjR7tReTXASw+Hh0zuFJdgLXAb9TM/2wy0aI/WHg8iSXJjmHlTdr7p1jcJKw8rr1YFX9xRwzj6uqW6pqa1VdwsrX/I9VNcsRrqq+Dzyf5G2Lm64FnpxjNiun71clOXfx/b+W9XmD8l5g5+LyTuCrcw1Osh34NHB9Vf14rrlU1br/AT7IyruS/wbcOuPc97HykuFx4F8Wfz64Dl//NcB9M898J7B/8bV/Bbhgxtl/BjwFHAD+FnjtxPPuZOX9gVdZOav5PeAXWHkX/pnFxwtnnH2Ilfepjv+d++s5vu/+uKzUxEY4jZc0A2OXmjB2qQljl5owdqkJY5eaMHapif8FzXlm7f7TeAAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALL0lEQVR4nO3dX+zddX3H8edrrUxATWGbRmkzMCFshDgwxKAuslhNKhLqhReQsXTTpDfbpMZES7gwu1uiMTWZ0TSIkNnABeIkRB0NasySSSzQMKAoHTpaqbbETI2SQON7F+eQlN9aIOf7Pd8efT8fSfP7nfM7p+/POekz53v+9PdJVSHp998fnO4FSJqGsUtNGLvUhLFLTRi71MT6KYclafnS/6ZNmwZd/8wzz1z4uocOHRo0+9lnnx10/SGG3m/PPPPMwtc9nbd7qKrKyc7PlG+9dY19165dg65/6aWXLnzdHTt2DJq9f//+QdcfYuj9duutty583dN5u4c6VewexktNGLvUhLFLTQyKPcmWJD9IcjDJzrEWJWl8C8eeZB3wOeB9wMXAdUkuHmthksY15JH9bcDBqnqyqp4D7gC2jrMsSWMbEvt5wIlv4h6en/ciSbYn2Zdk34BZkgYa8qGak72X9//eR6+q3cBu6Ps+u7QKhjyyHwZO/IjTRuDpYcuRtCxDYv8+cGGSC5KcAVwL3D3OsiSNbeHD+Ko6nuQfgH8H1gG3VNWjo61M0qgG/UeYqvo68PWR1iJpifwEndSEsUtNTPr/2bsa8l9UAa688sqFr7thw4ZBs0+noffb7/JtXwYf2aUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmhuziuinJt5McSPJokhvGXJikcQ35hZPHgY9V1YNJXgs8kGRvVT020tokjWjhR/aqOlJVD86//xVwgJPs4ippNYzyq6STnA9cBtx/kp9tB7aPMUfS4gbHnuQ1wFeAHVX1y7U/d8tmaTUMejU+yauYhb6nqu4aZ0mSlmHIq/EBvggcqKrPjLckScsw5JH9ncDfAO9Osn/+56qR1iVpZEP2Z/8PICOuRdIS+Qk6qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaGBx7knVJHkpyzxgLkrQcYzyy38BsB1dJK2zoXm8bgfcDN4+zHEnLMvSRfRfwceC3p7pAku1J9iXZN3CWpAGGbOx4NXC0qh54qctV1e6quryqLl90lqThhm7seE2SHwN3MNvg8cujrErS6BaOvapurKqNVXU+cC3wraq6frSVSRqV77NLTSy8ZfOJquo7wHfG+LskLYeP7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNTF0Y8cNSe5M8niSA0nePtbCJI1r6O+N/yzwzar6YJIzgLNGWJOkJVg49iSvA94F/C1AVT0HPDfOsiSNbchh/JuBY8CXkjyU5OYkZ6+9kFs2S6thSOzrgbcCn6+qy4BfAzvXXsgtm6XVMCT2w8Dhqrp/fvpOZvFLWkFDtmz+KXAoyUXzszYDj42yKkmjG/pq/D8Ce+avxD8J/N3wJUlahkGxV9V+wOfi0u8AP0EnNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITQ7ds/miSR5M8kuT2JK8ea2GSxrVw7EnOAz4CXF5VlwDrgGvHWpikcQ09jF8PnJlkPbO92Z8eviRJyzBkr7efAJ8GngKOAL+oqnvXXs4tm6XVMOQw/hxgK3AB8Cbg7CTXr72cWzZLq2HIYfx7gB9V1bGqeh64C3jHOMuSNLYhsT8FXJHkrCRhtmXzgXGWJWlsQ56z3w/cCTwI/Nf879o90rokjWzols2fBD450lokLZGfoJOaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qYmXjT3JLUmOJnnkhPPOTbI3yRPzr+csd5mShnolj+y3AlvWnLcTuK+qLgTum5+WtMJeNvaq+i7w8zVnbwVum39/G/CBkdclaWSL7gjzhqo6AlBVR5K8/lQXTLId2L7gHEkjGbT90ytRVbuZ7wGXpJY9T9LJLfpq/M+SvBFg/vXoeEuStAyLxn43sG3+/Tbga+MsR9KyvJK33m4H/hO4KMnhJB8G/hl4b5IngPfOT0taYS/7nL2qrjvFjzaPvBZJS+Qn6KQmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdamLRLZs/leTxJA8n+WqSDctdpqShFt2yeS9wSVW9BfghcOPI65I0soW2bK6qe6vq+Pzk94CNS1ibpBGN8Zz9Q8A3Rvh7JC3RoC2bk9wEHAf2vMRl3J9dWgELx55kG3A1sLmqTrnvuvuzS6thodiTbAE+AVxZVb8Zd0mSlmHRLZv/BXgtsDfJ/iRfWPI6JQ206JbNX1zCWiQtkZ+gk5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qYm8xC+GHX9Ycgz4n5e4yB8Dz0y0HGc7+/dx9p9W1Z+c7AeTxv5ykuyrqsud7Wxnj8/DeKkJY5eaWLXYdzvb2c5ejpV6zi5peVbtkV3Skhi71MRKxJ5kS5IfJDmYZOeEczcl+XaSA0keTXLDVLNPWMO6JA8luWfiuRuS3Jnk8fntf/uEsz86v78fSXJ7klcved4tSY4meeSE885NsjfJE/Ov50w4+1Pz+/3hJF9NsmEZs9c67bEnWQd8DngfcDFwXZKLJxp/HPhYVf05cAXw9xPOfsENwIGJZwJ8FvhmVf0Z8BdTrSHJecBHgMur6hJgHXDtksfeCmxZc95O4L6quhC4b356qtl7gUuq6i3AD4EblzT7RU577MDbgINV9WRVPQfcAWydYnBVHamqB+ff/4rZP/jzppgNkGQj8H7g5qlmzue+DngX8w06q+q5qvrfCZewHjgzyXrgLODpZQ6rqu8CP19z9lbgtvn3twEfmGp2Vd1bVcfnJ78HbFzG7LVWIfbzgEMnnD7MhMG9IMn5wGXA/ROO3QV8HPjthDMB3gwcA740fwpxc5KzpxhcVT8BPg08BRwBflFV904xe403VNWR+ZqOAK8/DWsA+BDwjSkGrULsOcl5k74fmOQ1wFeAHVX1y4lmXg0craoHppi3xnrgrcDnq+oy4Ncs7zD2RebPjbcCFwBvAs5Ocv0Us1dNkpuYPZXcM8W8VYj9MLDphNMbWfJh3YmSvIpZ6Huq6q6p5gLvBK5J8mNmT13eneTLE80+DByuqheOYu5kFv8U3gP8qKqOVdXzwF3AOyaafaKfJXkjwPzr0SmHJ9kGXA38dU30YZdViP37wIVJLkhyBrMXa+6eYnCSMHveeqCqPjPFzBdU1Y1VtbGqzmd2m79VVZM8wlXVT4FDSS6an7UZeGyK2cwO369Ictb8/t/M6XmB8m5g2/z7bcDXphqcZAvwCeCaqvrNVHOpqtP+B7iK2auS/w3cNOHcv2T2lOFhYP/8z1Wn4fb/FXDPxDMvBfbNb/u/AedMOPufgMeBR4B/Bf5wyfNuZ/b6wPPMjmo+DPwRs1fhn5h/PXfC2QeZvU71wr+5L0xxv/txWamJVTiMlzQBY5eaMHapCWOXmjB2qQljl5owdqmJ/wP8VyrGiUL55QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Defining our image\n",
    "x = np.zeros((1,28,28))\n",
    "x[0,:,18:20] = 1\n",
    "x[0,:,8:12] = 1\n",
    "plt.imshow(x[0], cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Defining our model\n",
    "conv = convLayer(kernelsize = 3, inputDim=1, outDim=3, stride=2, pad=1)\n",
    "\n",
    "#Sobel filter\n",
    "conv.W[0] = np.array([[-1,0,1],[-2,0,2],[-1,0,1]]).reshape((1,3,3))\n",
    "#Zero-weights\n",
    "conv.W[1] = np.zeros((1,3,3))\n",
    "#Gaussian blur\n",
    "conv.W[2] = np.array([[1/16,2/16,1/16],[2/16,4/16,2/16],[1/16,2/16,1/16]]).reshape((1,3,3))\n",
    "print(conv.W)\n",
    "\n",
    "x = conv(x)\n",
    "\n",
    "print(\"We can see that the result of the forward pass is as expected\")\n",
    "plt.imshow(x[0], cmap = \"gray\")\n",
    "plt.show()\n",
    "plt.imshow(x[1], cmap = \"gray\")\n",
    "plt.show()\n",
    "plt.imshow(x[2], cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking layers together\n",
    "\n",
    "As the layers has been defined abstract and modularized, we can build a network by stacking the moduels after each other. This code stacks 2 conv layers after one other followed by a maxpool layer, and checks the calculated gradient by comparing to finite differences. The activation function used is ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Defining our model\n",
    "conv1 = convLayer(kernelsize = 3, inputDim=1, outDim=3, stride=1, pad=1)\n",
    "conv2 = convLayer(kernelsize = 3, inputDim=3, outDim=1, stride=2, pad=1)\n",
    "\n",
    "maxpool = MaxPool()\n",
    "\n",
    "rl1 = ReLU()\n",
    "rl2 = ReLU()\n",
    "l2 = l2_loss()\n",
    "\n",
    "def model_forward(x):\n",
    "    logits = conv1(x)\n",
    "    logits = rl1(logits)\n",
    "    logits = conv2(logits)\n",
    "    logits = maxpool(logits)\n",
    "    logits = rl2(logits)\n",
    "    return logits\n",
    "\n",
    "def model_backward():\n",
    "    '''\n",
    "    The backward function demonstrates the chain rule.\n",
    "    The derivative delta is passed on layer by layer, to create an even longer\n",
    "    chain as one gets deeper in the network.\n",
    "    \n",
    "    Using the delta notation, finding the derivatives in a deep net is no more difficult than\n",
    "    in a shallow net.\n",
    "    '''\n",
    "    delta = l2.backward()\n",
    "    \n",
    "    delta = rl2.backward(delta)                \n",
    "    delta = maxpool.backward(delta)\n",
    "    \n",
    "    delta = conv2.backward(delta)\n",
    "\n",
    "    delta = rl1.backward(delta)\n",
    "    delta = conv1.backward(delta)\n",
    "\n",
    "#Defining our image\n",
    "x = np.random.normal(0,1,(1,28,28))\n",
    "#Our ''true'' downsampled image, that we will perform l2 norm-loss on\n",
    "y = np.random.normal(0,1,(1,7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "The following function performs a gradient check by comparing the calculated gradient with finite difference approximation. The result is of more interest than the details of the code, as it confirms that the backward pass in each model layer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing convlayer: 1, weights (0, 0)\n",
      "Assessing convlayer: 1, weights (1, 0)\n",
      "Assessing convlayer: 1, weights (2, 0)\n",
      "Assessing convlayer: 2, weights (0, 0)\n",
      "Assessing convlayer: 2, weights (0, 1)\n",
      "Assessing convlayer: 2, weights (0, 2)\n",
      "Finite differences and back-prop gradients agree. :)\n"
     ]
    }
   ],
   "source": [
    "#Performing gradient check\n",
    "for l, layer in enumerate([conv1, conv2]):\n",
    "    w_orig = layer.W.copy()\n",
    "    epsilon = 1e-7\n",
    "    for c in range(w_orig.shape[0]):\n",
    "        for d in range(w_orig.shape[1]):\n",
    "            print(f\"Assessing convlayer: {l+1}, weights {c, d}\" )\n",
    "            for i in range(w_orig.shape[2]):\n",
    "                for j in range(w_orig.shape[3]):\n",
    "                    #Approximated gradient\n",
    "                    orig = layer.W[c,d,i,j].copy()\n",
    "                    layer.W[c,d,i,j] = orig + epsilon\n",
    "                    logits = model_forward(x)\n",
    "                    cost1 = l2(logits, y)\n",
    "                    \n",
    "                    layer.W[c,d,i,j] = orig - epsilon\n",
    "                    logits = model_forward(x)\n",
    "                    cost2 = l2(logits, y)\n",
    "                    gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n",
    "                    \n",
    "                    # Actual gradient\n",
    "                    layer.W[c,d,i,j] = orig\n",
    "                    #Forward\n",
    "                    logits = model_forward(x)\n",
    "                    loss = l2(logits,y)\n",
    "\n",
    "                    #Backward\n",
    "                    model_backward()\n",
    "                    difference = gradient_approximation - layer.gradW[c, d, i, j]\n",
    "                    assert abs(difference) <= epsilon,\\\n",
    "                        f\"Calculated gradient is incorrect. \" \\\n",
    "                        f\"Approximation: {gradient_approximation}, actual gradient: {layer.gradW[c, d, i,j]}\\n\" \\\n",
    "                        f\"The incorrect gradient was in c: {c},d: {d} i: {i}, j: {j}\"\n",
    "                    \n",
    "                    \n",
    "print(\"Finite differences and back-prop gradients agree. :)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tdt4195",
   "language": "python",
   "name": "tdt4195"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
