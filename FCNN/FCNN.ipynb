{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected neural network\n",
    "\n",
    "This notebook will cover FCNN with forward pass, backward pass, different activation functions and differenc cost functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Weighted sum\n",
    "\n",
    "This section will cover neurons as weighted sums, without regard to activation functions.\n",
    "\n",
    "A single neuron in a fully connected neural network can be viewed as:\n",
    "\n",
    "$$z = \\sum{w_i x_i} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Forward pass\n",
    "**Given** multiple neurons in the both input and hidden layer, **find** the relationship between input and hidden layer in matrix notation.\n",
    "\n",
    "**Solution:** This can be described in matrix notation as such:\n",
    "\n",
    "$$\\vec{z} = W \\vec{x} + \\vec{b}$$\n",
    "\n",
    "If $\\vec{z}$ is a vector of length $m$ and $\\vec{x}$ is a vector of length $n$, then matrix $W$ is of shape $(m, n)$ and vector $\\vec{b}$ is of shape $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### Numerical example\n",
    "This following numerical example declares an input vector of length $n = 10$, a hidden layer of length $m$, weight matrix and bias vector and performs a forward pass. It does not regard multiple samples in a mini-batch, but could have easily been extended to cover that.\n",
    "\n",
    "The weight matrix is decleared to ones, and the input vector is given values evenly spaced in range $[0,9]$. The bias is also given linearly spaced values in range $[0,4]$. From the formulas, it should be clear that with a weight matrix of ones, each output vector element should contain the sum of the inputs plus the bias. This is confirmed in the numerical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "[45. 46. 47. 48. 49.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 10\n",
    "m = 5\n",
    "\n",
    "w = np.ones((m,n))\n",
    "b = np.linspace(0,4,(m))\n",
    "\n",
    "x = np.linspace(0, 9, (n))\n",
    "\n",
    "print(x)\n",
    "\n",
    "z = w @ x + b\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Backward pass\n",
    "\n",
    "The backward pass of such a linear layer can be calculated by diffrentiating with respect to the weights.\n",
    "\n",
    "In the one hidden neuron example this is simply:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial \\vec{w}} = \\vec{x}^T$$  \n",
    "\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial b} = 1$$\n",
    "\n",
    "\n",
    "The vector $\\vec{x}$ must be transposed as $\\vec{w}$ is a row vector with weights corresponding to the input column vector $\\vec{x}$\n",
    "\n",
    "This will be more interesting when we add activation functions later on, and I will wait with a numerical example untill then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Activation functions\n",
    "\n",
    "There are numerous activation functions. They provide non-linarities into the network so that it can learn non-linear functions.\n",
    "Here I will cover:\n",
    "\n",
    "* ReLU\n",
    "* Sigmoid\n",
    "* Softmax\n",
    "\n",
    "**Given** the three functions names, I want to **find** the forward and backward functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### ReLU\n",
    "\n",
    "ReLU can be simply described as $$f(z) = max(0, z)$$\n",
    "\n",
    "Differentiating ReLU gives f'(z) =  1 if $z>0$ else 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "Sigmoid is another non-linear activation function. It is expressed as:\n",
    "\n",
    "$$ f(z) = \\frac{1}{1 + e^{- z}}$$\n",
    "\n",
    "The sigmoid function is usefull as it is non-linear, it squashes the output between 0 and 1, and it has a simple derivative.\n",
    "\n",
    "$$f'(z) = \\frac{e^{-z}}{(1+ e^{-z})^2} = f(z) (1-f(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Softmax\n",
    "The softmax function is widely used for classification tasks. Given an input vector $\\vec{z}$ that represents $n$ classes, it gives an output vector $f(\\vec{z})$ which sums to one. Thus the output can be viewed as a probability distribution over all the classes.\n",
    "\n",
    "The function is given as follows:\n",
    "\n",
    "$$f(z) = \\frac{e^{z_j}}{\\sum{e^{z_j}}}$$\n",
    "\n",
    "The derivative of the function is a bit more difficult to derive:\n",
    "Using $y_k$ as the k'th element in the vector $\\vec{f(z)}$ \n",
    "\n",
    "$$\\frac{\\partial y_k}{\\partial z_j} = \\frac{\\partial}{\\partial z_j} \\frac{e^{z_k}}{\\sum{e^{z_i}}}$$\n",
    "\n",
    "using $g_k = e^{z_k}$ and $h_k = \\sum{e^{z_i}}$\n",
    "\n",
    "$$\\frac{\\partial g_k}{\\partial z_j} = e^{z_j}, j = k$$\n",
    "\n",
    "$$\\frac{\\partial g_k}{\\partial z_j} = 0, j \\neq k$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\frac{\\partial h_k}{\\partial z_j} = (e^{z_1} + e^{z_2} + ... + e^{z_j} + ... + e^{z_n})' = e^{z_j}$$\n",
    "\n",
    "\n",
    "then we can use\n",
    "\n",
    "$$\\frac{\\partial y_k}{\\partial z_j} = \\frac{g_k' h_k' - h_k' g_k}{h_k^2}$$\n",
    "\n",
    "Using the case $j = k$ from above I get:\n",
    "\n",
    "$$\\frac{\\partial y_k}{\\partial z_j} = \\frac{e^{z_j}}{\\sum{e^{z_i}}} \\big(\\frac{\\sum{e^{e_i} - e^{z_j}}}{\\sum{e^{z_i}}} \\big)= y_k* (1-y_k)$$\n",
    "\n",
    "if $j \\neq k$ we get a different solution:\n",
    "\n",
    "$$\\frac{\\partial y_k}{\\partial z_j}= \\frac{-e^{z_j} e^{z_k}}{(\\sum{e^{z_i}})^2} = - y_j * y_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it together\n",
    "\n",
    "A successfull forward pass will consist of both a weighted sum and an activation function\n",
    "\n",
    "$$a = f(wx + b)$$\n",
    "\n",
    "Derivating this with respect to w one gets the following:\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial w} = f'(wx + b) x^T$$\n",
    "\n",
    "and $$\\frac{\\partial a}{\\partial b} = f'(wx + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "The last activation layer will usually be passed to a cost function. For regression loss the mean square error is a common function. It is defined as:\n",
    "\n",
    "$$C(y) = \\frac{1}{N} \\sum{(y - y')^2}$$\n",
    "\n",
    "Where $y'$ is the ground truth.\n",
    "\n",
    "This is the function we want to differentiate in respect to. A common notation that utilizes the chain rule is to define:\n",
    "\n",
    "$$\\delta^l = \\frac{\\partial C}{\\partial z^l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Numerical task\n",
    "\n",
    "This task will with a practical example illustrate forward and backward pass with matrix notation and chain-rule.\n",
    "\n",
    "This following graph oulines a graphical representation of a neural network with two hidden layers and one output layer.\n",
    "![Task](fcnn_task.jpg)\n",
    "\n",
    "**Given** this network, **find** the matrix representation of the network, $\\frac{\\partial C}{\\partial b^2}$ and $\\frac{\\partial C}{\\partial w^1}$\n",
    "\n",
    "\n",
    "**Solutions**:\n",
    "I first calculate the solutions by hand:\n",
    "![Forward](fcnn-forward.jpg)\n",
    "![Backward](fcnn-backward.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Then I verify my results with a python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean square error given this network and input is: [[648.]]\n",
      "db2 =\n",
      "[[ 0]\n",
      " [36]]\n",
      "\n",
      "\n",
      "dw1 = \n",
      "[[  36  -36]\n",
      " [ 108 -108]\n",
      " [  72  -72]]\n",
      "As you can see, both the python script and calculation by hand gives the same results.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReLU_forward(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def ReLU_backward(intermediate_a):\n",
    "    return np.where(intermediate_a>0, 1, 0)\n",
    "\n",
    "def MSE(y, target):\n",
    "    return 1./2. * (y-target)**2\n",
    "\n",
    "layer_sizes = [3, 2, 1]\n",
    "\n",
    "input_size = 2\n",
    "\n",
    "w1 = np.array([[3,-3], [2,-2], [1,-1]])\n",
    "w2 = np.array([[-1,-3,2], [1,3,2]])\n",
    "w3 = np.array([[-1,1]])\n",
    "\n",
    "b1 = np.array([[1],[2],[3]])\n",
    "b2 = np.array([[1],[0]])\n",
    "b3 = np.array([[2]])\n",
    "\n",
    "x = np.array([[1],[-1]]) #Column vector\n",
    "target = 1\n",
    "\n",
    "#Forward pass\n",
    "a1 = ReLU_forward(w1 @ x  + b1)\n",
    "#print(f\"a1: {a1}\")\n",
    "a2 = ReLU_forward(w2 @ a1 + b2)\n",
    "#print(f\"a2: {a2}\")\n",
    "a3 = ReLU_forward(w3 @ a2 + b3)\n",
    "#print(f\"a3: {a3}\")\n",
    "loss = MSE(a3, target)\n",
    "print(f\"The mean square error given this network and input is: {loss}\")\n",
    "\n",
    "##Backward pass\n",
    "\n",
    "#Derivative of cost function multiplied with derivative of activation function\n",
    "delta3 = (a3 - target)* ReLU_backward(a3) \n",
    "\n",
    "dw3 = delta3 @ a2.transpose()\n",
    "db3 = delta3\n",
    "\n",
    "delta2 = ReLU_backward(a2) * (w3.transpose() @ delta3)\n",
    "\n",
    "dw2 = delta2 @ a1.transpose()\n",
    "db2 = delta2\n",
    "\n",
    "delta1 = ReLU_backward(a1) * (w2.transpose() @ delta2)\n",
    "\n",
    "dw1 = delta1 @ x.transpose()\n",
    "db1 = delta1\n",
    "\n",
    "print(f\"db2 =\\n{db2}\\n\\n\\ndw1 = \\n{dw1}\")\n",
    "\n",
    "#Weight update\n",
    "w1 -= dw1\n",
    "w2 -= dw2\n",
    "w3 -= dw3\n",
    "\n",
    "\n",
    "b1 -= db1\n",
    "b2 -= db2\n",
    "b3 -= db3\n",
    "\n",
    "\n",
    "\n",
    "print(\"As you can see, both the python script and calculation by hand gives the same results.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tdt4195",
   "language": "python",
   "name": "tdt4195"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
