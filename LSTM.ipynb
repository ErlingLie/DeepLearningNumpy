{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "A LSTM-, long short term memory, cell is a more sofisticated type of RNN. RNNs are normally used to treat serialized data, and LSTMs are better at treating long term dependencies than traditional vanilla LSTMs.\n",
    "\n",
    "**Note** As I already showed how to calculate fully connected neural networks by hand, I will not do so here. Instead all numerical examples are coded with python and numpy.\n",
    "\n",
    "**Given:**\n",
    "The LSTM equations:\n",
    "\n",
    "$$c_v = h_{t-1} + x_t$$\n",
    "$$f_t = \\sigma (W_f c_v + b_f)$$\n",
    "$$i_t = \\sigma (W_i c_v + b_i)$$\n",
    "$$\\hat{c_t} = tanh (W_c c_v + b_c)$$\n",
    "$$o_t = \\sigma (W_o c_v + b_o)$$\n",
    "\n",
    "\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\hat{c_t}$$\n",
    "$$h_t = o_t \\odot tanh(c_t)$$\n",
    "\n",
    "\n",
    "and the input text string \"Neural Networks are fun\"\n",
    "\n",
    "**Find:**\n",
    "* The forward pass that passes every component of the input through an LSTM cell. (Multiple cell pass through)\n",
    "\n",
    "* The backward pass that propagates the loss through a single cell. (Single example pass through)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "Here I implement and show a simple forward pass in an LSTM. The forward pass through multiple timesteps is done by feeding the lstm with the c, and h output from the previous iteration along with the x-vector of the current iteration. This is all done in a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing word 'Neural' through LSTM\n",
      "Passing word 'networks' through LSTM\n",
      "Passing word 'are' through LSTM\n",
      "\n",
      "\n",
      "Predicted vector: [[0.29021869]\n",
      " [0.17640258]\n",
      " [0.28724179]\n",
      " [0.24613694]]\n",
      "Loss: 1.237120547437759\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Onehot encode data\n",
    "string = \"Neural networks are fun\"\n",
    "vocab = list(set(string.split()))\n",
    "word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx_to_word = {i:w for i,w in enumerate(vocab)}\n",
    "x = []\n",
    "for word in string.split():\n",
    "    x_i = np.zeros((len(vocab),1))\n",
    "    x_i[word_to_idx[word]] = 1\n",
    "    x.append(x_i)\n",
    "\n",
    "    \n",
    "cell_size = len(vocab)\n",
    "\n",
    "W_f = np.random.normal(0,1,(cell_size, cell_size))\n",
    "b_f = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "W_i = np.random.normal(0,1,(cell_size, cell_size))\n",
    "b_i = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "W_g = np.random.normal(0,1,(cell_size, cell_size))\n",
    "b_g = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "W_o = np.random.normal(0,1,(cell_size, cell_size))\n",
    "b_o = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "\n",
    "def sigmoid(x, w, b):\n",
    "    z = w@x + b\n",
    "    return np.exp(z)/(np.exp(z) + 1)\n",
    "\n",
    "def tanh(x,w,b):\n",
    "    z = w@x + b\n",
    "    return np.tanh(z)\n",
    "\n",
    "def lstm(x, c_t, h_t):\n",
    "    cv = h_t + x_i\n",
    "    f_t = sigmoid(cv, W_f, b_f)\n",
    "    i_t = sigmoid(cv,W_i, b_i)\n",
    "    g_t = tanh(cv, W_g, b_g)\n",
    "    o_t = sigmoid(cv, W_o, b_o)\n",
    "    c_t = c_t*f_t + i_t*g_t\n",
    "    h_t = o_t*np.tanh(c_t)\n",
    "    return c_t, h_t\n",
    "\n",
    "def softmax(x):\n",
    "    sf = np.exp(x)\n",
    "    sf /= sf.sum(keepdims=True)\n",
    "    return sf\n",
    "def cross_entropy_loss(x,targets):\n",
    "    ce = targets * np.log(x)\n",
    "    ce = ce.sum()\n",
    "    return -ce\n",
    "#Initializing cell state and hidden state to zero\n",
    "c_t = np.zeros((cell_size,1))\n",
    "h_t = np.zeros((cell_size,1))\n",
    "#Doing the actual forward pass\n",
    "for x_i in x[:-1]:\n",
    "    print(f\"Passing word '{idx_to_word[np.argmax(x_i)]}' through LSTM\")\n",
    "    c_t, h_t = lstm(x, c_t, h_t)\n",
    "    \n",
    "y_gt = x[-1]\n",
    "y_pred = softmax(h_t)\n",
    "print(f\"\\n\\nPredicted vector: {y_pred}\")\n",
    "loss = cross_entropy_loss(y_pred, y_gt)\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "To utilize the chain rule for backward pass I must calculate the derivatives within the LSTM. I will not write up the derivative of sigmoid/tanh here as I believe that belongs to a different topic. With the derivatives provided here, the derivative with respect to the cost function can be found using the chain rule.\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial c_t} = o_t \\odot (1-tan(c_t)^2)$$\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial o_t} = tanh(c_t)$$\n",
    "\n",
    "$$\\frac{\\partial c_t}{\\partial c_{t-1}} = f_t$$\n",
    "$$\\frac{\\partial c_t}{\\partial f_t} = c_{t-1}$$\n",
    "$$\\frac{\\partial c_t}{\\partial i_t} = g_t$$\n",
    "$$\\frac{\\partial c_t}{\\partial g_t} = i_t$$\n",
    "\n",
    "All the gates i, f, g, o will have the same general structure for derivative. I will define the general derivative of these with $y_t$ as output.\n",
    "\n",
    "$$\\frac{\\partial y_t}{\\partial W} = \\frac{\\partial y_t}{\\partial z_t} x_t^T$$\n",
    "$$\\frac{\\partial y_t}{\\partial b} = \\frac{\\partial y_t}{\\partial z_t}$$\n",
    "\n",
    "\n",
    "To illustrate the backpropagation I will pass through only one timestep, but to do so through multiple time steps, one must simply cache the internal activations along the way and let \n",
    "$$\\frac{\\partial C}{\\partial c_{t-1}} = \\frac{\\partial C}{\\partial h_{t-1}}\\frac{\\partial h_{t-1}}{\\partial c_{t-1}} + \\frac{\\partial C}{\\partial c_t}\\frac{\\partial c_t}{\\partial c_{t-1}}$$.\n",
    "\n",
    "Thus backwards pass through more than one cell should not bee too difficult.\n",
    "\n",
    "\n",
    "\n",
    "I implemented the above equations in numpy and used the finite difference gradient approximation test from assignment 2 to verify my calculated gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(a):\n",
    "    return a*(1-a)\n",
    "\n",
    "#Defining LSTM with forward and backward pass\n",
    "class LSTM():\n",
    "    def __init__(self,cell_size):\n",
    "        #Initializing weights\n",
    "        self.W_f = np.random.normal(0,1,(cell_size, cell_size))\n",
    "        self.b_f = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "        self.W_i = np.random.normal(0,1,(cell_size, cell_size))\n",
    "        self.b_i = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "        self.W_g = np.random.normal(0,1,(cell_size, cell_size))\n",
    "        self.b_g = np.random.normal(0,1,(cell_size,1))\n",
    "\n",
    "        self.W_o = np.random.normal(0,1,(cell_size, cell_size))\n",
    "        self.b_o = np.random.normal(0,1,(cell_size,1))\n",
    "    def forward(self,x, c_t, h_t):\n",
    "        self.c_t0 = c_t\n",
    "        self.cv = h_t + x_i\n",
    "        self.f_t = sigmoid(self.cv, self.W_f, self.b_f)\n",
    "        self.i_t = sigmoid(self.cv,self.W_i, self.b_i)\n",
    "        self.g_t = tanh(self.cv, self.W_g, self.b_g)\n",
    "        self.o_t = sigmoid(self.cv, self.W_o, self.b_o)\n",
    "        \n",
    "        self.c_t = c_t*self.f_t + self.i_t*self.g_t\n",
    "        self.h_t = self.o_t*np.tanh(self.c_t)\n",
    "        return self.c_t, self.h_t\n",
    "    def backward(self, y_pred, y_gt):\n",
    "        delta = y_pred - y_gt\n",
    "\n",
    "        d_ct = delta*self.o_t*(1-np.tanh(self.c_t)**2) #Tanh backward\n",
    "        d_ot = delta*np.tanh(self.c_t)\n",
    "\n",
    "        d_ft = d_ct*self.c_t0\n",
    "        d_it = d_ct*self.g_t\n",
    "        d_gt = d_ct*self.i_t\n",
    "\n",
    "        self.d_wi = d_it*sigmoid_backward(self.i_t)@self.cv.transpose()\n",
    "        self.d_wf = d_ft*sigmoid_backward(self.f_t)@self.cv.transpose()\n",
    "        self.d_wg = d_gt*(1-self.g_t**2)@self.cv.transpose() #tanh backward\n",
    "        self.d_wo = d_ot*sigmoid_backward(self.o_t)@self.cv.transpose()\n",
    "\n",
    "        self.d_bi = d_it*sigmoid_backward(self.i_t)\n",
    "        self.d_bf = d_ft*sigmoid_backward(self.f_t)\n",
    "        self.d_bg = d_gt*(1-self.g_t**2)\n",
    "        self.d_bo = d_ot*sigmoid_backward(self.o_t)\n",
    "        \n",
    "    def weights(self):\n",
    "        return [self.W_f, self.W_o, self.W_g, self.W_i]\n",
    "    def weight_grads(self):\n",
    "        return [self.d_wf, self.d_wo, self.d_wg, self.d_wi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull for weights 0\n",
      "Successfull for weights 1\n",
      "Successfull for weights 2\n",
      "Successfull for weights 3\n"
     ]
    }
   ],
   "source": [
    "#Data vector\n",
    "x = np.zeros((cell_size,1))\n",
    "x[0] = 1\n",
    "\n",
    "lstm = LSTM(cell_size)\n",
    "\n",
    "#Initializing cell state and hidden state to zero\n",
    "c_t0 = np.zeros((cell_size,1))\n",
    "h_t0 = np.zeros((cell_size,1))\n",
    "\n",
    "c_t1, h_t1 = lstm.forward(x, c_t0, h_t0)\n",
    "\n",
    "y_pred = softmax(h_t1)\n",
    "\n",
    "lstm.backward(y_pred, y_gt)\n",
    "\n",
    "\n",
    "epsilon = 1e-4\n",
    "for layer_idx,(w, dw) in enumerate(zip(lstm.weights(),lstm.weight_grads())):\n",
    "    \n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            \n",
    "            orig = w[i, j].copy()\n",
    "            w[i, j] = orig + epsilon\n",
    "            c_t1, h_t1 = lstm.forward(x, c_t0, h_t0)\n",
    "            y_pred = softmax(h_t1)\n",
    "            cost1 = cross_entropy_loss(y_pred, y_gt)\n",
    "            \n",
    "            w[i, j] = orig - epsilon\n",
    "            c_t1, h_t1 = lstm.forward(x, c_t0, h_t0)\n",
    "            y_pred = softmax(h_t1)\n",
    "            cost2 = cross_entropy_loss(y_pred, y_gt)\n",
    "            gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n",
    "            \n",
    "            w[i, j] = orig\n",
    "            \n",
    "            # Actual gradient\n",
    "            c_t1, h_t1 = lstm.forward(x, c_t0, h_t0)\n",
    "            lstm.backward(softmax(h_t1), y_pred)\n",
    "            difference = gradient_approximation - dw[i,j]\n",
    "            assert abs(difference) <= epsilon**2,\\\n",
    "                f\"Calculated gradient is incorrect. \" \\\n",
    "                f\"Layer IDX = {layer_idx}, i={i}, j={j}.\\n\" \\\n",
    "                f\"Approximation: {gradient_approximation}, actual gradient: {dw[i, j]}\\n\" \\\n",
    "                f\"If this test fails there could be errors in your loss function, \" \\\n",
    "                f\"forward function or backward function\"\n",
    "    print(f\"Successfull for weights {layer_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tdt4195",
   "language": "python",
   "name": "tdt4195"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
